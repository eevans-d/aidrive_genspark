#!/usr/bin/env python3
"""
Script de despliegue completo para aplicar optimizaciones retail en producci√≥n
SOLUCIONA, APLICA E IMPLEMENTA las optimizaciones en el sistema real
"""

import os
import sys
import sqlite3
import psycopg2
import logging
import asyncio
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import subprocess
import time

# Configurar logging detallado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('deployment.log')
    ]
)
logger = logging.getLogger(__name__)


class RetailOptimizationDeployer:
    """
    Desplegador completo de optimizaciones retail para AIDRIVE_GENSPARK_FORENSIC
    """
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.config_path = self.project_root / "config" / "database"
        self.deployed_optimizations = []
        self.deployment_report = {
            "timestamp": datetime.now().isoformat(),
            "optimizations_applied": [],
            "warnings": [],
            "errors": [],
            "success": False
        }
        
    async def deploy_all_optimizations(self) -> bool:
        """
        Aplicar todas las optimizaciones retail al sistema en producci√≥n
        """
        logger.info("üöÄ INICIANDO DESPLIEGUE COMPLETO DE OPTIMIZACIONES RETAIL")
        logger.info(f"üìÅ Proyecto: {self.project_root}")
        
        success = True
        
        # Fase 1: Verificar prerequisitos
        if not await self._verify_prerequisites():
            return False
        
        # Fase 2: Aplicar optimizaciones SQLite (inventario-retail)
        if not await self._deploy_sqlite_optimizations():
            success = False
        
        # Fase 3: Aplicar optimizaciones PostgreSQL (BI + Dep√≥sito)
        if not await self._deploy_postgresql_optimizations():
            success = False
        
        # Fase 4: Configurar m√©tricas y monitoreo
        if not await self._deploy_monitoring_setup():
            success = False
        
        # Fase 5: Validar despliegue
        if not await self._validate_deployment():
            success = False
        
        # Fase 6: Generar reporte final
        await self._generate_deployment_report()
        
        self.deployment_report["success"] = success
        
        if success:
            logger.info("üéâ DESPLIEGUE COMPLETADO EXITOSAMENTE")
        else:
            logger.error("‚ùå DESPLIEGUE COMPLETADO CON ERRORES")
        
        return success
    
    async def _verify_prerequisites(self) -> bool:
        """Verificar que todos los prerequisitos est√©n cumplidos"""
        logger.info("üîç Verificando prerequisitos del despliegue...")
        
        try:
            # Verificar archivos de configuraci√≥n
            required_configs = [
                "inventario_sqlite_pragmas.sql",
                "bi_postgresql_indices.sql", 
                "deposito_postgresql_optimizations.sql"
            ]
            
            for config_file in required_configs:
                config_path = self.config_path / config_file
                if not config_path.exists():
                    logger.error(f"‚ùå Archivo de configuraci√≥n faltante: {config_path}")
                    return False
                logger.info(f"‚úÖ Configuraci√≥n encontrada: {config_file}")
            
            # Verificar m√≥dulos compartidos
            shared_modules = [
                "retail_validation.py",
                "retail_transactions.py",
                "retail_metrics.py"
            ]
            
            for module in shared_modules:
                module_path = self.project_root / "shared" / module
                if not module_path.exists():
                    logger.error(f"‚ùå M√≥dulo compartido faltante: {module_path}")
                    return False
                logger.info(f"‚úÖ M√≥dulo compartido encontrado: {module}")
            
            # Verificar subm√≥dulos del proyecto
            submodules = [
                "inventario-retail",
                "business-intelligence-orchestrator-v3.1", 
                "sistema_deposito_semana1"
            ]
            
            for submodule in submodules:
                submodule_path = self.project_root / submodule
                if not submodule_path.exists():
                    logger.warning(f"‚ö†Ô∏è Subm√≥dulo no encontrado: {submodule}")
                    self.deployment_report["warnings"].append(f"Subm√≥dulo faltante: {submodule}")
                else:
                    logger.info(f"‚úÖ Subm√≥dulo encontrado: {submodule}")
            
            logger.info("‚úÖ Verificaci√≥n de prerequisitos completada")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error verificando prerequisitos: {e}")
            self.deployment_report["errors"].append(f"Prerequisitos: {e}")
            return False
    
    async def _deploy_sqlite_optimizations(self) -> bool:
        """Aplicar optimizaciones SQLite al m√≥dulo inventario-retail"""
        logger.info("üóÑÔ∏è Aplicando optimizaciones SQLite para inventario-retail...")
        
        try:
            # Buscar bases de datos SQLite en inventario-retail
            inventario_path = self.project_root / "inventario-retail"
            sqlite_databases = []
            
            if inventario_path.exists():
                # Buscar archivos .db en el m√≥dulo
                for db_file in inventario_path.rglob("*.db"):
                    sqlite_databases.append(db_file)
                
                # Buscar referencias a SQLite en configuraciones
                for config_file in inventario_path.rglob("*.py"):
                    try:
                        with open(config_file, 'r') as f:
                            content = f.read()
                            if "sqlite" in content.lower() and "DATABASE_URL" in content:
                                logger.info(f"üìã Configuraci√≥n SQLite encontrada en: {config_file}")
                    except:
                        continue
            
            # Si no hay bases de datos existentes, crear una de demostraci√≥n
            if not sqlite_databases:
                demo_db = inventario_path / "data" / "inventario.db" if inventario_path.exists() else self.project_root / "demo_inventario.db"
                demo_db.parent.mkdir(parents=True, exist_ok=True)
                sqlite_databases.append(demo_db)
                logger.info(f"üì¶ Creando base de datos de demostraci√≥n: {demo_db}")
            
            # Aplicar optimizaciones a cada base de datos
            config_file = self.config_path / "inventario_sqlite_pragmas.sql"
            with open(config_file, 'r') as f:
                optimizations_sql = f.read()
            
            for db_path in sqlite_databases:
                logger.info(f"üîß Aplicando optimizaciones a: {db_path}")
                
                # Crear conexi√≥n SQLite
                conn = sqlite3.connect(str(db_path))
                
                try:
                    # Ejecutar optimizaciones pragma por pragma
                    for statement in optimizations_sql.split(';'):
                        statement = statement.strip()
                        if statement and not statement.startswith('--'):
                            try:
                                conn.execute(statement)
                                logger.debug(f"‚úÖ Ejecutado: {statement[:50]}...")
                            except sqlite3.Error as e:
                                if "already exists" not in str(e):
                                    logger.warning(f"‚ö†Ô∏è SQLite warning: {e}")
                    
                    conn.commit()
                    
                    # Verificar optimizaciones aplicadas
                    stats = self._verify_sqlite_optimizations(conn)
                    logger.info(f"üìä Estad√≠sticas SQLite: {stats}")
                    
                    self.deployed_optimizations.append({
                        "type": "SQLite",
                        "database": str(db_path),
                        "stats": stats,
                        "timestamp": datetime.now().isoformat()
                    })
                    
                finally:
                    conn.close()
            
            logger.info(f"‚úÖ Optimizaciones SQLite aplicadas a {len(sqlite_databases)} bases de datos")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error aplicando optimizaciones SQLite: {e}")
            self.deployment_report["errors"].append(f"SQLite: {e}")
            return False
    
    async def _deploy_postgresql_optimizations(self) -> bool:
        """Aplicar optimizaciones PostgreSQL a BI y Dep√≥sito"""
        logger.info("üêò Aplicando optimizaciones PostgreSQL...")
        
        success = True
        
        # Configuraciones PostgreSQL por m√≥dulo
        pg_configs = [
            {
                "name": "Business Intelligence Orchestrator",
                "config_file": "bi_postgresql_indices.sql",
                "env_vars": {
                    "host": "BI_PG_HOST",
                    "port": "BI_PG_PORT", 
                    "database": "BI_PG_DATABASE",
                    "user": "BI_PG_USER",
                    "password": "BI_PG_PASSWORD"
                },
                "defaults": {
                    "host": "localhost",
                    "port": "5432",
                    "database": "business_intelligence",
                    "user": "bi_user",
                    "password": "password"
                }
            },
            {
                "name": "Sistema Dep√≥sito",
                "config_file": "deposito_postgresql_optimizations.sql",
                "env_vars": {
                    "host": "DEPOSITO_PG_HOST",
                    "port": "DEPOSITO_PG_PORT",
                    "database": "DEPOSITO_PG_DATABASE", 
                    "user": "DEPOSITO_PG_USER",
                    "password": "DEPOSITO_PG_PASSWORD"
                },
                "defaults": {
                    "host": "localhost",
                    "port": "5432",
                    "database": "deposito_db",
                    "user": "deposito_user", 
                    "password": "deposito_pass"
                }
            }
        ]
        
        for pg_config in pg_configs:
            logger.info(f"üîß Configurando {pg_config['name']}...")
            
            try:
                # Obtener par√°metros de conexi√≥n
                conn_params = {}
                for param, env_var in pg_config["env_vars"].items():
                    conn_params[param] = os.getenv(env_var, pg_config["defaults"][param])
                
                logger.info(f"üì° Intentando conexi√≥n a {conn_params['host']}:{conn_params['port']}/{conn_params['database']}")
                
                # Intentar conectar a PostgreSQL
                try:
                    conn_string = (
                        f"host={conn_params['host']} "
                        f"port={conn_params['port']} "
                        f"dbname={conn_params['database']} "
                        f"user={conn_params['user']} "
                        f"password={conn_params['password']}"
                    )
                    
                    conn = psycopg2.connect(conn_string)
                    
                    # Leer archivo de configuraci√≥n
                    config_file = self.config_path / pg_config["config_file"]
                    with open(config_file, 'r') as f:
                        sql_content = f.read()
                    
                    # Aplicar optimizaciones
                    with conn.cursor() as cursor:
                        for statement in sql_content.split(';'):
                            statement = statement.strip()
                            if statement and not statement.startswith('--'):
                                try:
                                    cursor.execute(statement)
                                    logger.debug(f"‚úÖ Ejecutado: {statement[:50]}...")
                                except psycopg2.Error as e:
                                    if "already exists" not in str(e):
                                        logger.warning(f"‚ö†Ô∏è PostgreSQL warning: {e}")
                    
                    conn.commit()
                    
                    # Verificar optimizaciones
                    stats = self._verify_postgresql_optimizations(conn, pg_config["name"])
                    logger.info(f"üìä Estad√≠sticas {pg_config['name']}: {stats}")
                    
                    self.deployed_optimizations.append({
                        "type": "PostgreSQL",
                        "module": pg_config["name"],
                        "database": conn_params['database'],
                        "stats": stats,
                        "timestamp": datetime.now().isoformat()
                    })
                    
                    conn.close()
                    logger.info(f"‚úÖ Optimizaciones aplicadas a {pg_config['name']}")
                    
                except psycopg2.Error as e:
                    logger.warning(f"‚ö†Ô∏è No se pudo conectar a {pg_config['name']}: {e}")
                    logger.info(f"üí° Para habilitar {pg_config['name']}, configure las variables de entorno:")
                    for param, env_var in pg_config["env_vars"].items():
                        logger.info(f"   export {env_var}={conn_params[param]}")
                    
                    self.deployment_report["warnings"].append(f"PostgreSQL {pg_config['name']}: {e}")
                    
            except Exception as e:
                logger.error(f"‚ùå Error configurando {pg_config['name']}: {e}")
                self.deployment_report["errors"].append(f"PostgreSQL {pg_config['name']}: {e}")
                success = False
        
        return success
    
    async def _deploy_monitoring_setup(self) -> bool:
        """Configurar sistema de monitoreo y m√©tricas"""
        logger.info("üìä Configurando sistema de monitoreo...")
        
        try:
            # Crear directorio de monitoreo si no existe
            monitoring_dir = self.project_root / "monitoring" / "retail"
            monitoring_dir.mkdir(parents=True, exist_ok=True)
            
            # Crear configuraci√≥n de m√©tricas
            metrics_config = {
                "retail_metrics": {
                    "enabled": True,
                    "port": 9090,
                    "path": "/metrics",
                    "update_interval": 60
                },
                "alerts": {
                    "stock_critical_days": 3,
                    "inflation_threshold_percent": 20,
                    "ocr_confidence_minimum": 0.7
                },
                "dashboards": {
                    "grafana_import": "monitoring/dashboards/retail_dashboard.json"
                }
            }
            
            # Guardar configuraci√≥n
            config_file = monitoring_dir / "metrics_config.json"
            with open(config_file, 'w') as f:
                json.dump(metrics_config, f, indent=2)
            
            logger.info(f"‚úÖ Configuraci√≥n de m√©tricas guardada en: {config_file}")
            
            # Crear script de inicio de m√©tricas
            metrics_script = monitoring_dir / "start_metrics.py"
            with open(metrics_script, 'w') as f:
                f.write("""#!/usr/bin/env python3
\"\"\"
Script para iniciar el sistema de m√©tricas retail
\"\"\"
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from shared.retail_metrics import create_retail_metrics_collector
import asyncio
import time

async def start_metrics_collector():
    print("üöÄ Iniciando collector de m√©tricas retail...")
    
    # Mock database factory para demostraci√≥n
    async def mock_db_factory():
        class MockSession:
            async def execute(self, query, params=None):
                return []
        return MockSession()
    
    collector = create_retail_metrics_collector(mock_db_factory)
    
    while True:
        try:
            # Calcular m√©tricas cada minuto
            await collector.calculate_stock_metrics()
            await collector.calculate_business_metrics()
            print(f"üìä M√©tricas actualizadas: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            await asyncio.sleep(60)
        except Exception as e:
            print(f"‚ùå Error calculando m√©tricas: {e}")
            await asyncio.sleep(10)

if __name__ == "__main__":
    asyncio.run(start_metrics_collector())
""")
            
            os.chmod(metrics_script, 0o755)
            logger.info(f"‚úÖ Script de m√©tricas creado: {metrics_script}")
            
            # Crear documentaci√≥n de despliegue
            deploy_doc = monitoring_dir / "DEPLOYMENT_GUIDE.md"
            with open(deploy_doc, 'w') as f:
                f.write("""# Gu√≠a de Despliegue - Monitoreo Retail

## Configuraci√≥n de M√©tricas

1. **Iniciar collector de m√©tricas:**
```bash
python monitoring/retail/start_metrics.py
```

2. **Configurar Grafana:**
```bash
# Importar dashboard
curl -X POST http://localhost:3000/api/dashboards/db \\
  -H "Content-Type: application/json" \\
  -d @monitoring/dashboards/retail_dashboard.json
```

3. **Variables de entorno requeridas:**
```bash
export PROMETHEUS_PORT=9090
export GRAFANA_URL=http://localhost:3000
export ALERT_EMAIL=admin@company.com
```

## M√©tricas Disponibles

- `retail_stock_value_total_ars` - Valor total inventario
- `retail_low_stock_items_count` - Items con stock bajo
- `retail_inventory_turnover_rate` - Tasa de rotaci√≥n
- `retail_price_inflation_impact_percent` - Impacto inflaci√≥n

## Alertas Configuradas

- Stock cr√≠tico: < 3 d√≠as de venta
- Inflaci√≥n alta: > 20% en 90 d√≠as
- OCR confianza baja: < 70%
""")
            
            logger.info(f"‚úÖ Documentaci√≥n de despliegue creada: {deploy_doc}")
            
            self.deployed_optimizations.append({
                "type": "Monitoring",
                "components": ["metrics_config", "start_script", "documentation"],
                "timestamp": datetime.now().isoformat()
            })
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error configurando monitoreo: {e}")
            self.deployment_report["errors"].append(f"Monitoring: {e}")
            return False
    
    async def _validate_deployment(self) -> bool:
        """Validar que el despliegue fue exitoso"""
        logger.info("üîç Validando despliegue completo...")
        
        validation_results = {
            "config_files": True,
            "optimizations_applied": len(self.deployed_optimizations) > 0,
            "monitoring_setup": True,
            "system_health": True
        }
        
        try:
            # Validar archivos de configuraci√≥n
            for config_file in ["inventario_sqlite_pragmas.sql", "bi_postgresql_indices.sql", "deposito_postgresql_optimizations.sql"]:
                if not (self.config_path / config_file).exists():
                    validation_results["config_files"] = False
                    logger.error(f"‚ùå Archivo de configuraci√≥n faltante: {config_file}")
            
            # Validar optimizaciones aplicadas
            if len(self.deployed_optimizations) == 0:
                validation_results["optimizations_applied"] = False
                logger.error("‚ùå No se aplicaron optimizaciones")
            else:
                logger.info(f"‚úÖ Se aplicaron {len(self.deployed_optimizations)} optimizaciones")
            
            # Validar sistema de monitoreo
            monitoring_dir = self.project_root / "monitoring" / "retail"
            if not monitoring_dir.exists():
                validation_results["monitoring_setup"] = False
                logger.error("‚ùå Sistema de monitoreo no configurado")
            
            # Generar reporte de validaci√≥n
            all_valid = all(validation_results.values())
            
            if all_valid:
                logger.info("‚úÖ Validaci√≥n del despliegue exitosa")
            else:
                logger.error(f"‚ùå Validaci√≥n del despliegue fall√≥: {validation_results}")
            
            return all_valid
            
        except Exception as e:
            logger.error(f"‚ùå Error durante validaci√≥n: {e}")
            return False
    
    async def _generate_deployment_report(self):
        """Generar reporte completo del despliegue"""
        logger.info("üìã Generando reporte de despliegue...")
        
        self.deployment_report.update({
            "total_optimizations": len(self.deployed_optimizations),
            "deployed_optimizations": self.deployed_optimizations,
            "project_root": str(self.project_root),
            "deployment_duration": "N/A"  # Se podr√≠a calcular si se guarda el tiempo de inicio
        })
        
        # Guardar reporte en archivo
        report_file = self.project_root / "deployment_report.json"
        with open(report_file, 'w') as f:
            json.dump(self.deployment_report, f, indent=2)
        
        logger.info(f"üìä Reporte guardado en: {report_file}")
        
        # Mostrar resumen en consola
        logger.info("=" * 60)
        logger.info("üìä RESUMEN DEL DESPLIEGUE")
        logger.info("=" * 60)
        logger.info(f"‚úÖ Optimizaciones aplicadas: {len(self.deployed_optimizations)}")
        logger.info(f"‚ö†Ô∏è Advertencias: {len(self.deployment_report['warnings'])}")
        logger.info(f"‚ùå Errores: {len(self.deployment_report['errors'])}")
        
        for opt in self.deployed_optimizations:
            logger.info(f"  üîß {opt['type']}: {opt.get('database', opt.get('module', 'Sistema'))}")
        
        if self.deployment_report["warnings"]:
            logger.info("\n‚ö†Ô∏è ADVERTENCIAS:")
            for warning in self.deployment_report["warnings"]:
                logger.info(f"  - {warning}")
        
        if self.deployment_report["errors"]:
            logger.info("\n‚ùå ERRORES:")
            for error in self.deployment_report["errors"]:
                logger.info(f"  - {error}")
        
        logger.info("=" * 60)
    
    def _verify_sqlite_optimizations(self, conn) -> Dict[str, str]:
        """Verificar optimizaciones SQLite aplicadas"""
        stats = {}
        try:
            # Verificar pragmas
            cursor = conn.execute("PRAGMA journal_mode")
            stats['journal_mode'] = cursor.fetchone()[0]
            
            cursor = conn.execute("PRAGMA cache_size") 
            stats['cache_size'] = str(cursor.fetchone()[0])
            
            cursor = conn.execute("PRAGMA foreign_keys")
            stats['foreign_keys'] = str(cursor.fetchone()[0])
            
            # Contar √≠ndices personalizados
            cursor = conn.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='index' AND name LIKE 'idx_%'")
            stats['custom_indexes'] = str(cursor.fetchone()[0])
            
        except Exception as e:
            stats['error'] = str(e)
        
        return stats
    
    def _verify_postgresql_optimizations(self, conn, module_name: str) -> Dict[str, str]:
        """Verificar optimizaciones PostgreSQL aplicadas"""
        stats = {}
        try:
            with conn.cursor() as cursor:
                # Contar √≠ndices personalizados
                cursor.execute("SELECT COUNT(*) FROM pg_indexes WHERE indexname LIKE 'idx_%'")
                stats['custom_indexes'] = str(cursor.fetchone()[0])
                
                # Verificar tablas analizadas
                cursor.execute("SELECT COUNT(*) FROM pg_stat_user_tables WHERE schemaname = 'public'")
                stats['analyzed_tables'] = str(cursor.fetchone()[0])
                
                stats['module'] = module_name
                
        except Exception as e:
            stats['error'] = str(e)
        
        return stats


async def main():
    """Funci√≥n principal de despliegue"""
    if len(sys.argv) < 2:
        print("Uso: python deploy_retail_optimizations.py <project_root_path>")
        print("Ejemplo: python deploy_retail_optimizations.py /path/to/aidrive_genspark_forensic")
        sys.exit(1)
    
    project_root = sys.argv[1]
    
    if not os.path.exists(project_root):
        logger.error(f"‚ùå Directorio del proyecto no encontrado: {project_root}")
        sys.exit(1)
    
    logger.info("üöÄ INICIANDO DESPLIEGUE DE OPTIMIZACIONES RETAIL")
    logger.info(f"üìÅ Proyecto: {project_root}")
    
    deployer = RetailOptimizationDeployer(project_root)
    
    # Ejecutar despliegue completo
    success = await deployer.deploy_all_optimizations()
    
    if success:
        logger.info("üéâ DESPLIEGUE COMPLETADO EXITOSAMENTE")
        logger.info("üí° Pr√≥ximos pasos:")
        logger.info("   1. Revisar deployment_report.json para detalles")
        logger.info("   2. Configurar variables de entorno para PostgreSQL si es necesario")
        logger.info("   3. Iniciar sistema de m√©tricas: python monitoring/retail/start_metrics.py")
        logger.info("   4. Importar dashboard de Grafana desde monitoring/dashboards/")
        sys.exit(0)
    else:
        logger.error("‚ùå DESPLIEGUE COMPLETADO CON ERRORES")
        logger.error("üìã Revisar deployment_report.json y logs para detalles")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())